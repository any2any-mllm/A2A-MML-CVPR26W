<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>A2A-MML 2026</title>
  <meta name="description" content="Workshop On Any-to-Any Multimodal Learning"/>
  <link rel="stylesheet" href="css/styles.css" />
</head>

<body>
  <nav class="nav" aria-label="Primary">
    <div class="container nav-inner">
      <a class="brand" href="#">
        <img class="logo" src="./logo.png" alt="A2A-MML logo" />
        <span>A2A-MML | CVPR 2026</span>
      </a>

      <!-- Desktop menu -->
      <div class="menu" aria-label="Desktop Menu">
        <a href="#about">About</a>
        <a href="#call-for-papers">Call for Papers</a>
        <a href="#speakers">Speakers</a>
        <a href="#schedule">Tentative Schedule</a>
        <a href="#organizers">Organizers</a>
      </div>

      <!-- Mobile toggle -->
      <button id="navToggle" class="nav-toggle" type="button" aria-label="Open menu" aria-controls="mobileMenu" aria-expanded="false">
        <span class="bars" aria-hidden="true">
          <span></span><span></span><span></span>
        </span>
      </button>
    </div>
  </nav>

  <!-- Mobile 2-level menu -->
  <div id="mobileMenu" class="mobile-menu" aria-hidden="true">
    <div class="mm-overlay" data-close="1"></div>
    <div class="mm-panel" role="dialog" aria-modal="true" aria-label="Menu">
      <div class="mm-head">
        <div class="mm-title">Menu</div>
        <button class="mm-close" type="button" aria-label="Close menu" data-close="1">‚úï</button>
      </div>

      <div class="mm-groups">
        <details class="mm-group" open>
          <summary>
            Workshop
            <span class="mm-caret" aria-hidden="true">‚Ä∫</span>
          </summary>
          <div class="mm-links">
            <a class="mm-link" href="#call-for-papers">Call for Papers</a>
            <a class="mm-link" href="#schedule">Tentative Schedule</a>
          </div>
        </details>

        <details class="mm-group">
          <summary>
            People
            <span class="mm-caret" aria-hidden="true">‚Ä∫</span>
          </summary>
          <div class="mm-links">
            <a class="mm-link" href="#speakers">Speakers</a>
            <a class="mm-link" href="#organizers">Organizers</a>
          </div>
        </details>
      </div>
    </div>
  </div>

  <header id="top" class="hero">
    <div class="container wrap">
      <span class="badge">CVPR 2026 Workshop</span>
      <h1>Workshop On Any-to-Any Multimodal Learning</h1>
      <div class="hero-meta">
        <span class="hero-meta-item"><span class="hero-meta-icon" aria-hidden="true">üìÖ</span> June 2026</span>
        <span class="hero-meta-item"><span class="hero-meta-icon" aria-hidden="true">üìç</span> Denver, CO</span>
      </div>
      <div class="cta">
        <a class="cta-btn" href="https://openreview.net/group?id=thecvf.com/CVPR/2026/Workshop/A2A-MML#tab-recent-activity" target="_blank" rel="noreferrer">Submit Paper</a>
        <a class="cta-btn" href="#about">Learn More</a>
      </div>
    </div>
  </header>

  <main class="content">
    <section id="about">
      <div class="container">
        <h2 class="section-title">About the Workshop</h2>
        The recent surge of multimodal large models has brought unprecedented progress in connecting language, vision, audio, and beyond.
        Yet, despite their impressive performance, most existing systems remain constrained to fixed modality 
        pairs, lacking the flexibility to generalize or reason across arbitrary modality combinations. 
        The first edition of the Any-to-Any Multimodal Learning (A2A-MML) workshop aims to explore the next frontier of multimodal intelligence‚Äîdeveloping systems capable of understanding, aligning, transforming, and generating across any set of modalities. We organize the discussion around three foundational pillars:
        <!-- <ul class="bullets">
          <li><em>Multimodal representation learning</em>, which seeks to learn generalizable and disentangled representations that capture both shared and modality-specific information;</li>
          <li><em>Multimodal transformation</em>, which investigates architectures and principles enabling seamless translation, fusion, and adaptation across heterogeneous modalities;</li>
          <li><em>Multimodal collaboration</em>, which studies how modalities cooperate, complement, or conflict with each other to achieve coherent reasoning and world modeling.</li>
        </ul> -->

        <p>For the latest papers and datasets, please refer to <a href="https://github.com/any2any-mllm/awesome-any2any" target="_blank" ><b>Awesome-Any-to-Any-Generation</b></a>. This repository is regularly updated and provides valuable resources for your submission.</p>
        <!-- By bringing together researchers from vision, language, audio, 3D, robotics, and cognitive science, this workshop aims to establish a roadmap toward any-to-any multimodal intelligence, where machines can perceive, reason, and communicate across modalities as fluidly as humans do. -->

        <h3>Topics and Themes</h3>
        <p>We welcome all relevant submissions in the area of multimodal learning, with emphasis on any-to-any multimodal intelligence, such as:</p>
        <ul class="bullets">
          <li>Multimodal Representation Learning</li>
            <li>Multimodal Transformation</li>
            <li>Multimodal Synergistic Collaboration</li>
            <li>Benchmarking and Evaluation for Any-to-Any Multimodal Learning</li>
        </ul>

        <p>Other topics include, but are not limited to:</p>
        <ul class="bullets">
          <li>Unified multimodal foundation and agentic models.</li>
            <li>Representation learning for embodied and interactive systems.</li>
            <li>Integration of underexplored modalities and cognitive perspectives on multimodal perception and reasoning.</li>
        </ul>

        <!-- <h3>About</h3>
        <p>The recent surge of multimodal large models has brought strong progress in connecting language, vision, audio, and beyond. Yet most existing systems remain constrained to fixed modality pairs, lacking flexibility to generalize or reason across arbitrary combinations. The Any-to-Any Multimodal Learning workshop aims to explore systems that can understand, align, transform, and generate across any set of modalities. We organize the discussion around three pillars: representation learning, transformation, and collaboration.</p>
        <p>For the latest papers and datasets, please refer to <a href="https://github.com/any2any-mllm/awesome-any2any" target="_blank" ><b>Awesome-Any-to-Any-Generation</b></a>. This repository is regularly updated and provides valuable resources for your submission.</p> -->
      </div>
    </section>


    <section id="call-for-papers">
      <div class="container">
        <h2 class="section-title">Call for Papers</h2>

        <h3>Important Dates</h3>
        <p class="dates-note">Important Dates for Review Process are as follows</p>

        <div class="dates-grid" aria-label="Important Dates">
          <div class="date-card">
            <p class="date is-deadline">Mar 01, 2026 AOE</p>
            <p class="label">Workshop Paper Submission Deadline</p>
          </div>

          <div class="date-card">
            <p class="date">Mar 19, 2026</p>
            <p class="label">Workshop Paper Notification Date</p>
          </div>

          <div class="date-card">
            <p class="date">Apr 10, 2026</p>
            <p class="label">Program, Camera-ready, Videos Uploaded</p>
          </div>
        </div>

        <h3>Paper Submission and Acceptance</h3>
        <p>We welcome technical, position, or perspective papers related to the topics outlined below. All submissions must be written in English, follow the official CVPR proceedings format, and adhere to the double-blind review policy.</p>

        <div class="submit-row">
          <a class="submit-btn" href="https://openreview.net/group?id=thecvf.com/CVPR/2026/Workshop/A2A-MML#tab-recent-activity" target="_blank" rel="noreferrer">Submit Paper</a>
        </div>

        <ul class="bullets">
          <li><strong>Tiny or Short Papers (2‚Äì4 pages)</strong> - We invite concise papers that present implementations and evaluations of unpublished but insightful ideas, moderate yet self-contained theoretical analyses, follow-up experiments, re-analyses of prior work, or new perspectives on existing research.</li>
            <li><strong>Regular Papers (up to 8 pages, including figures and tables)</strong> - We encourage submissions introducing original methods, novel research visions, applications, or discussions of open challenges in multimodal learning.</li>
        </ul>
        <p>We accept both archival and non-archival paper submissions; authors should indicate the submission type during submission.</p>
        <p>A Best Paper Award will be presented based on reviewer scores and the workshop committee‚Äôs evaluation.</p>
        <p>All accepted papers will be presented as posters during the workshop, and some of them will be selected for short oral presentations. Poster sessions will be conducted onsite with dedicated time for interactive discussions. For remote attendees, we will offer a virtual poster gallery and live Q&amp;A channels to ensure inclusive engagement.</p>
      </div>
    </section>

    <section id="speakers">
      <div class="container">
        <h2 class="section-title">Invited Speakers</h2>
        <div style="height:16px"></div>
        <div id="speakersGrid" class="grid" aria-label="Invited Speakers"></div>
      </div>
    </section>

    <section id="schedule">
      <div class="container">
        <h2 class="section-title">Tentative Schedule</h2>
        <div style="height:16px"></div>

        <table class="schedule" aria-label="Workshop Schedule">
          <thead>
            <tr>
              <th style="width:180px">Time</th>
              <th>Schedule</th>
              <th style="width:160px">Speaker</th>
            </tr>
          </thead>
          <tbody>
            <tr class="section-row"><td colspan="3">Morning Schedule</td></tr>
            <tr><td>TBD</td><td>Introduction and opening remarks</td><td>-</td></tr>
            <tr><td>TBD</td><td>Keynote Talk 1</td><td>TBD</td></tr>
            <tr><td>TBD</td><td>Keynote Talk 2</td><td>TBD</td></tr>
            <tr><td>TBD</td><td>Oral Presentations</td><td>-</td></tr>
            <tr><td>TBD</td><td>Coffee Break</td><td>-</td></tr>
            <tr><td>TBD</td><td>Keynote Talk 3</td><td>TBD</td></tr>
            <tr><td>TBD</td><td>Keynote Talk 4</td><td>TBD</td></tr>
            <tr><td>TBD</td><td>Poster Session 1 (Interactive) + Virtual Gallery</td><td>-</td></tr>
            <tr><td>TBD</td><td>Lunch Break</td><td>-</td></tr>

            <tr class="section-row"><td colspan="3">Afternoon Schedule</td></tr>
            <tr><td>TBD</td><td>Keynote Talk 5</td><td>TBD</td></tr>
            <tr><td>TBD</td><td>Keynote Talk 6</td><td>TBD</td></tr>
            <tr><td>TBD</td><td>Poster Session 2 (Interactive) + Virtual Gallery</td><td>-</td></tr>
            <tr><td>TBD</td><td>Coffee Break</td><td>-</td></tr>
            <tr><td>TBD</td><td>Keynote Talk 7</td><td>TBD</td></tr>
            <tr><td>TBD</td><td>Panel Discussion</td><td>TBD</td></tr>
            <tr><td>TBD</td><td>Closing Remarks + Best Paper Award</td><td>TBD</td></tr>
          </tbody>
        </table>
      </div>
    </section>

    <section id="organizers">
      <div class="container">
        <h2 class="section-title">Organization</h2>
        <h3>Steering Committe</h3>
        <div style="height:12px"></div>
        <div id="organizersGrid" class="grid" aria-label="Organizers"></div>

        <h3>Executive Commitee</h3>
        <div style="height:12px"></div>
        <div id="contributorsGrid" class="grid" aria-label="Contributors"></div>
      </div>
    </section>

  </main>

  <footer>
    <div class="container">
      <div class="muted">CVPR 2026 Workshop on Any-to-Any Multimodal Learning</div>
      <div class="muted">A2A-MML</div>
    </div>
  </footer>

  <div id="modal" class="modal" role="dialog" aria-modal="true" aria-label="Profile">
    <div class="modal-panel">
      <div class="modal-head">
        <div>
          <h3 id="modalTitle" class="modal-title"></h3>
          <div id="modalSub" class="modal-sub"></div>
        </div>
        <button id="modalClose" class="modal-close" type="button" aria-label="Close">‚úï</button>
      </div>
      <div class="modal-body">
        <div class="modal-photo"><img id="modalImg" alt="" /></div>
        <div id="modalBio" class="modal-text"></div>
      </div>
      <div class="modal-actions">
        <a id="modalHome" class="home-btn" href="#" target="_blank" rel="noreferrer"></a>
      </div>
    </div>
  </div>

  <script>
    const ORGANIZERS = [
      { name:"Shengqiong Wu", affil:"University of Oxford", img:"./shengqiongwu.png", homepage:"https://sqwu.top/", bio:`Shengqiong Wu (she/her) is a research fellow at Oxford University, having previously received a Ph.D. degree at the National University of Singapore, under the supervision of Prof. Tat-Seng Chua. Her research interests include multimodal learning and large vision-language foundation models. She has been recognized with several prestigious awards, including the Google Ph.D. Fellowship, Baidu Scholarship, and Bytedance Scholarship. She has published over 10 papers in top-tier conferences, e.g., ICLR, ICML, NeurIPS, and CVPR. She has co-organized multiple workshops and grand challenges, such as ACM MM and WACV.

Email: shengqiong.wu@cs.ox.ac.uk
`},

      { name:"Wei Dai", affil:"MIT Media Lab", img:"./Wei_Dai.png", homepage:"https://dd.works/", bio:`Wei Dai (he/him) is a Ph.D. student at MIT Media Lab under the supervision of Prof. Paul Liang. His research interests lie in the field of multimodal learning, particularly multimodal LLMs, and their applications to healthcare. Before joining MIT, he was a master's student at Stanford University, supervised by Prof. Li Fei-Fei and Prof. Ehsan Adeli. His work has been published at top conferences, including NeurIPS, ICML, and MICCAI. His research is supported by NSF Graduate Research Fellowships (GRFP) and ORCD Seed Fund.

Email: dvdai@mit.edu
`},

      { name:"Han Lin", affil:"UNC Chapel Hill", img:"./Han_Lin.png", homepage:"https://hl-hanlin.github.io/", bio:`Han Lin (he/him) is a third-year Ph.D. student at the University of North Carolina at Chapel Hill, supervised by Prof. Mohit Bansal. His research interests include image and video generation, multimodal learning, and LLMs. Before joining UNC, he was a master's student at Columbia University, supervised by Prof. Shih-Fu Chang, Prof.  Matei Ciocarlie, and Prof. Shuran Song. He has over 13 papers in top-tier conferences such as ICLR, ICML, NeurIPS, CVPR, and COLM.

Email: hanlincs@cs.unc.edu
`},

      { name:"Yichen Li", affil:"MIT", img:"./Yichen.jpeg", homepage:"https://people.csail.mit.edu/yichenl/", bio:`Yichen Li (he/him) is a Ph.D. student at MIT, working with Prof. Antonio Torralba. Her research interests lie in the field of multimodal learning and foundational learning paradigms agnostic to data modalities. She received the Robert Shillman Graduate Fellowship and has published works at top-tier conferences, including CVPR, ICCV, ICLR, ICML, and ECCV. She also served as a core organizer for multiple workshops at ECCV and RSS.
`},

      { name:"Chenyu Monica Wang", affil:"MIT", img:"./Chenyu.jpeg", homepage:"https://chenyuwang-monica.github.io/", bio:`Chenyu (Monica) Wang (she/her) is a Ph.D. student at MIT, advised by Prof. Tommi Jaakkola. Her research interests lie broadly in deep generative models, reinforcement learning, multi-modal learning, and AI for science. During her PhD, she was a research intern at Meta FAIR and Genentech. Her research is supported by The Citadel GQS PhD Fellowship. She has published work at top-tier conferences, including NeurIPS and ICLR.
`},

      { name:"Sharut Gupta", affil:"MIT", img:"./Sharut.jpg", homepage:"https://www.mit.edu/~sharut/", bio:`Sharut Gupta (she/her) is a Ph.D. student at MIT, working with Prof. Phillip Isola and Prof. Stefanie Jegelka. Prior to her PhD, she received her Bachelor‚Äôs and Master‚Äôs (Dual) degrees from the Indian Institute of Technology Delhi (IIT Delhi). Her research interests focus on multi-modal representation learning, robustness, and out-of-distribution generalization. Sharut is a recipient of the MIT Presidential Fellowship and the MathWorks Engineering Fellowship, and has published work at top-tier conferences, including NeurIPS, ICLR, and ICML.

Email: sharut@mit.edu
`},

      { name:"Roman Bachmann", affil:"EPFL", img:"./Roman.jpg", homepage:"https://roman-bachmann.github.io/", bio:`Roman Bachmann (he/him) is a Ph.D. student at EPFL, advised by Prof. Amir Zamir. Starting in early 2026, he will join Apple as a research scientist. His research focus is on developing scalable objectives for training any-to-any models across a large and diverse set of modalities, as well as training flexible tokenizers for generative modeling. He has published works at top-tier conferences, including CVPR, ICCV, ECCV, NeurIPS, ICML, and SIGGRAPH.

Email: roman.bachmann@epfl.ch
`},

      { name:"Elisa Ricci", affil:"University of Trento", img:"./Elisa.png", homepage:"https://eliricci.eu/", bio:`Elisa Ricci (she/her) is a Professor at the University of Trento. She is also the Coordinator of the Doctoral Program in Information Engineering and Computer Science at Trento, a member of ELLIS, and an IAPR Fellow. Her research spans the intersection of computer vision, deep learning, and robotics perception, with an emphasis on domain adaptation, continual learning, and self-supervised learning from visual and multimodal data.
`},

      { name:"Hao Fei", affil:"University of Oxford", img:"./feihao.jpg", homepage:"https://haofei.vip/", bio:`Hao is a senior postdoctoral researcher at the University of Oxford. His research focuses on the theme of Multimodal Foundational Modeling with an emphasis on Generative Models, Multimodal Large Language Models, Agents, and World Models. He is particularly interested in exploring unified and advanced generalists towards human-level capacity and cognition for various scenarios and domains. Previously, he was a senior research fellow at the National University of Singapore, an associate researcher at Skywork AI Singapore, and SEA AI lab, respectively.
`},
    ];

    const CONTRIBUTORS = [
      { 
        name:"Shiqi Dai (Web Design)", affil:"Gunn High School", img:"./shichi.jpg", homepage:"https://cvpr-2026-workshop-jpxj74c2g-shiqis-projects-6be7ecea.vercel.app/", bio:`Shiqi Dai (she/her) is a high school student at Gunn High School under the supervision of Prof. Paul Liang. Her research interests lie in the field of multimodal learning, particularly multimodal LLMs, and their applications to healthcare. 

Email: sd53709@pausd.us
`},
      {
        name: "Yanlin Li", affil: "National University of Singapore", img: "./Yanlin.jpg", homepage: "https://liyanlin06.github.io/", bio: `Yanlin Li (he/him) is a Master student in Artificial Intelligence at the School of Computing, National University of Singapore. His research interests lie in unified multimodal understanding and generation, human-centered AGI.

Email: yanlin.li@u.nus.edu
`},
      {
        name: "Minghui Guo", affil: "National University of Singapore", img: "./Minghui.jpg", homepage: "https://guominghui07.github.io/", bio: `I am an MSc student at National University of Singapore, and my research interests include unified any-to-any multimodal LLMs, representation learning, and video understanding and generation, as well as AI for Science and data-driven scientific discovery.

Email: mguo77@u.nus.edu
`},
    ];

    const SPEAKERS = [
      { name:"Paul Liang", affil:"MIT", img:"./Paul_Liang.jpeg", homepage:"https://www.mit.edu/~ppliang/", bio:`Paul Pu Liang is an Assistant Professor at the MIT Media Lab and MIT EECS, and leads the Multisensory Intelligence research group, where he investigates how machines can learn from and reason across diverse sensory modalities to engage with the real world. He received his PhD in Machine Learning from Carnegie Mellon University. His research focuses on multimodal representation learning, modality transformation via unified architectures, and interactive systems that bridge human and machine experience. He is dedicated to mentoring the next generation of researchers and promoting broader participation in AI.`},

      { name:"Manling Li", affil:"Northwestern University", img:"./Manling.jpeg", homepage:"https://limanling.github.io/", bio:`Manling Li is an Assistant Professor of Computer Science at Northwestern University, where she leads the Machine Learning and Language (MLL) Lab. Her research explores how machines can reason, plan, and act across modalities‚Äîbridging language, vision, audio, robotics, and embodied interaction‚Äîto build trustworthy, compositional, and long-horizon intelligent systems. She holds a PhD from the University of Illinois Urbana-Champaign and was a post-doctoral researcher at Stanford University. Prof. Manling has received many awards, including the MIT Technology Review 35 Under 35, Microsoft Research PhD Fellowship, and the ACL 2024 Outstanding Paper Award.`},

      { name:"Mohit Bansal", affil:"University of North Carolina, Chapel Hill", img:"./Mohit.png", homepage:"https://www.cs.unc.edu/~mbansal/", bio:`Mohit Bansal is the John R. & Louise S. Parker Distinguished Professor and the Director of the MURGe-Lab (UNC-NLP Group) in the Computer Science department at UNC Chapel Hill. His research expertise is in natural language processing and multimodal machine learning, with a particular focus on multimodal generative models, grounded and embodied semantics, and interpretable, efficient, and generalizable deep learning. He is an AAAI Fellow and recipient of the Presidential Early Career Award for Scientists and Engineers (PECASE), IIT Kanpur Young Alumnus Award, and outstanding paper awards at ACL, CVPR, and TMLR. His service includes EMNLP and CoNLL Program Co-Chair, ACM Doctoral Dissertation Award Committee, and Associate/Action Editor for TACL, CL, and IEEE/ACM.`},

      { name:"Zhedong Zheng", affil:"University of Macau", img:"./zhedongzheng.webp", homepage:"https://zdzheng.xyz/", bio:`Zhedong Zheng is an Assistant Professor with the University of Macau. He received the Ph.D. degree from the University of Technology Sydney in 2021 and the B.S. degree from Fudan University in 2016. He was a postdoctoral research fellow at the School of Computing, National University of Singapore. He received the IEEE Circuits and Systems Society Outstanding Young Author Award of 2021. His research interests include AIGC, Data-centric AI, and Spatial Intelligence. He actively serves the academic community, acting as a Senior PC for IJCAI and AAAI, an Area Chair for ACM MM‚Äô24, ACM MM‚Äô25 and ICASSP‚Äô25, and the Publication Chair for ACM MM‚Äô25 and AVSS‚Äô25.`},

      { name:"Yossi Gandelsman", affil:"Reqe", img:"./Yossi.png", homepage:"https://yossigandelsman.github.io/", bio:`Yossi Gandelsman is a research scientist at Reve. Starting Fall 2026, he will join the Toyota Technological Institute at Chicago (TTIC) as an assistant professor. His research interests focus on interpretability and model mechanisms across computer vision and language. He received his PhD from the University of California, Berkeley, worked on the Perception Team at Google DeepMind, and holds an M.Sc. from the Weizmann Institute of Science.`},

      { name:"Georgia Gkioxari", affil:"Caltech", img:"./Georgia.jpeg", homepage:"https://gkioxari.github.io/", bio:`Georgia Gkioxari is an Assistant Professor in the Computing & Mathematical Sciences department at Caltech. Previously, she worked as a research scientist at Meta FAIR. She obtained her PhD under Jitendra Malik at UC Berkeley, and completed her undergraduate studies in Greece at the National Technical University of Athens. Her research focuses on advanced visual perception, including 2D & 3D spatial representation and reasoning, transforming images into structured multi-modal outputs. She has received numerous honors, including the PAMI Young Researcher Award, Google Faculty Award, and Okawa Research Award.`},

      { name:"Saining Xie", affil:"NYU", img:"./Saining.png", homepage:"https://sainingxie.com/", bio:`Saining Xie is an Assistant Professor of Computer Science at NYU‚Äôs Courant Institute and a member of the NYU Center for Data Science. He earned his Ph.D. in Computer Science & Engineering from UC San Diego and previously worked as a research scientist at Facebook AI Research. His research develops robust, scalable visual-intelligence systems which bridge visual perception, representation learning, and commonsense reasoning.`}
    ];

    function escapeHtml(s){
      return String(s)
        .replaceAll("&","&amp;")
        .replaceAll("<","&lt;")
        .replaceAll(">","&gt;")
        .replaceAll('"',"&quot;")
        .replaceAll("'","&#039;");
    }

    function cardHTML(p){
      return `
        <div class="card" role="button" tabindex="0" data-name="${escapeHtml(p.name)}">
          <div class="img">
            <img src="${escapeHtml(p.img)}" alt="${escapeHtml(p.name)}" />
            <div class="chev" aria-hidden="true"><span>‚Üí</span></div>
          </div>
          <div class="meta">
            <p class="name">${escapeHtml(p.name)}</p>
            <p class="affil">${escapeHtml(p.affil || "")}</p>
          </div>
        </div>
      `;
    }

    function renderGrid(elId, arr, roleLabel){
      const el = document.getElementById(elId);
      el.innerHTML = arr.map(cardHTML).join("");
      el.querySelectorAll(".card").forEach(card=>{
        const name = card.getAttribute("data-name");
        const p = arr.find(x=>x.name===name);
        const open = ()=>openModal(p, roleLabel);
        card.addEventListener("click", open);
        card.addEventListener("keydown", e=>{
          if(e.key==="Enter" || e.key===" "){ e.preventDefault(); open(); }
        });
      });
    }

    const modal = document.getElementById("modal");
    const modalClose = document.getElementById("modalClose");
    const modalTitle = document.getElementById("modalTitle");
    const modalSub = document.getElementById("modalSub");
    const modalImg = document.getElementById("modalImg");
    const modalBio = document.getElementById("modalBio");
    const modalHome = document.getElementById("modalHome");

    function openModal(p, roleLabel){
      modalTitle.textContent = p.name || "";
      modalSub.textContent = p.affil || "";
      modalImg.src = p.img || "";
      modalImg.alt = p.name || "";

      const bioText = (p.bio || "").trim();
      modalBio.innerHTML = bioText
        ? bioText
            .split(/\n\s*\n/g)
            .map(par => `<p>${escapeHtml(par).replace(/\n/g, "<br>")}</p>`)
            .join("")
        : "";

      if(p.homepage){
        modalHome.href = p.homepage;
        modalHome.textContent = `The ${roleLabel}'s Homepage`;
        modalHome.style.display = "inline-flex";
      }else{
        modalHome.href = "#";
        modalHome.textContent = `The ${roleLabel}'s Homepage`;
        modalHome.style.display = "none";
      }
      modal.classList.add("open");
      document.body.style.overflow = "hidden";
      modalClose.focus();
    }

    function closeModal(){
      modal.classList.remove("open");
      document.body.style.overflow = "";
    }

    modalClose.addEventListener("click", closeModal);
    modal.addEventListener("click", e=>{ if(e.target === modal) closeModal(); });
    document.addEventListener("keydown", e=>{ if(e.key === "Escape" && modal.classList.contains("open")) closeModal(); });

    renderGrid("speakersGrid", SPEAKERS, "Speaker");
    renderGrid("organizersGrid", ORGANIZERS, "Organizer");
    renderGrid("contributorsGrid", CONTRIBUTORS, "Contributor");

    /* ===== Mobile menu JS ===== */
    const navToggle = document.getElementById("navToggle");
    const mobileMenu = document.getElementById("mobileMenu");

    function openMobileMenu(){
      mobileMenu.classList.add("open");
      mobileMenu.setAttribute("aria-hidden", "false");
      navToggle.setAttribute("aria-expanded", "true");
      document.body.style.overflow = "hidden";
    }

    function closeMobileMenu(){
      mobileMenu.classList.remove("open");
      mobileMenu.setAttribute("aria-hidden", "true");
      navToggle.setAttribute("aria-expanded", "false");
      document.body.style.overflow = "";
      navToggle.focus();
    }

    navToggle.addEventListener("click", ()=>{
      const isOpen = mobileMenu.classList.contains("open");
      if(isOpen) closeMobileMenu();
      else openMobileMenu();
    });

    mobileMenu.addEventListener("click", (e)=>{
      const t = e.target;
      if(t && t.getAttribute && t.getAttribute("data-close")==="1") closeMobileMenu();
      if(t && t.classList && (t.classList.contains("mm-link") || t.classList.contains("mm-submit"))){
        closeMobileMenu();
      }
    });

    document.addEventListener("keydown", (e)=>{
      if(e.key === "Escape" && mobileMenu.classList.contains("open")) closeMobileMenu();
    });

    window.addEventListener("resize", ()=>{
      if(window.matchMedia("(min-width: 861px)").matches && mobileMenu.classList.contains("open")){
        closeMobileMenu();
      }
    });
  </script>

  <script>
    function updateBodyPaddingForNav(){
      const nav = document.querySelector('.nav');
      if(!nav) return;
      const h = Math.ceil(nav.getBoundingClientRect().height);
      document.body.style.paddingTop = h + 'px';
    }
    window.addEventListener('load', updateBodyPaddingForNav);
    window.addEventListener('resize', updateBodyPaddingForNav);
  </script>
</body>
</html>
