# Any-to-Any Mmultimodal Learning Workshop on CVPR 2026

The recent surge of multimodal large models has brought strong progress in connecting language, vision, audio, and beyond. Yet most existing systems remain constrained to fixed modality pairs, lacking flexibility to generalize or reason across arbitrary combinations. The Any-to-Any Multimodal Learning workshop aims to explore systems that can understand, align, transform, and generate across any set of modalities. We organize the discussion around three pillars: representation learning, transformation, and collaboration.

## [**Call for Papers**](https://a2a-mml-2026.vercel.app/)

We welcome all relevant submissions in the area of multimodal learning, with emphasis on any-to-any multimodal intelligence, such as:

- Multimodal Representation Learning
- Multimodal Transformation
- Multimodal Synergistic Collaboration
- Benchmarking and Evaluation for Any-to-Any Multimodal Learning

Other topics include, but are not limited to:

- Unified multimodal foundation and agentic models.
- Representation learning for embodied and interactive systems.
- Integration of underexplored modalities and cognitive perspectives on multimodal perception and reasoning.


## Useful link
A github Repo includes the related works: [**Awesome-Any-to-Any-Generation**](https://github.com/any2any-mllm/awesome-any2any).


